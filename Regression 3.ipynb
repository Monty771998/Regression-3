{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f18e70f8-75d6-42c3-834a-9f11ae2357e7",
   "metadata": {},
   "source": [
    "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100297d0-8389-4f60-b467-220bfd603d7f",
   "metadata": {},
   "source": [
    "Ridge Regression, also known as Tikhonov regularization, extends traditional linear regression by including an L2 regularization term in the cost function. This term penalizes the sum of the squared coefficients, which helps to constrain the model and reduce the impact of less important predictors. The key benefit of Ridge Regression is its ability to handle multicollinearity (when predictors are highly correlated) and improve the model's stability and performance on new data by preventing overfitting.\n",
    "\n",
    "Ordinary Least Squares (OLS) regression, in contrast, focuses solely on minimizing the residual sum of squares between the observed data and the predictions made by the model. It seeks to find the coefficients that best fit the training data without any penalty on the size of these coefficients. While OLS can achieve a good fit to the training data, it may result in a model that is too complex and sensitive to the noise in the data, leading to poor generalization to new data.\n",
    "\n",
    "In summary, Ridge Regression addresses the limitations of OLS by adding a regularization term that reduces the magnitude of the coefficients and thus simplifies the model. This makes Ridge Regression more robust, particularly in situations where there are many predictors or multicollinearity is present. OLS, on the other hand, does not include this regularization, which can lead to overfitting and less reliable predictions in some cases.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4802a1-56b3-4e23-a9a7-b83e48037972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3575de9d-9cc3-4f1d-8568-3f1847bc4876",
   "metadata": {},
   "source": [
    "Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a76065-7d4d-4efb-8302-acf907551f4e",
   "metadata": {},
   "source": [
    "Ridge Regression shares several assumptions with traditional linear regression, with the added assumption of handling regularization. The key assumptions are:\n",
    "\n",
    "1)Linearity: The relationship between the predictors and the response variable is assumed to be linear. This means that the model assumes a straight-line relationship between each predictor and the response variable.\n",
    "\n",
    "2)Independence: The residuals (errors) should be independent of each other. This assumption implies that the observations are not correlated.\n",
    "\n",
    "3)Homoscedasticity: The variance of the residuals should be constant across all levels of the independent variables. This means the spread of residuals should be the same for all predicted values.\n",
    "\n",
    "4)Normality of Residuals: For inference purposes, it is assumed that the residuals are normally distributed. This assumption is less critical for Ridge Regression compared to OLS but is important for hypothesis testing.\n",
    "\n",
    "5)Multicollinearity Handling: Ridge Regression assumes that there may be multicollinearity among predictors. It specifically addresses this by penalizing large coefficients to stabilize the estimates when predictors are highly correlated.\n",
    "\n",
    "In summary, Ridge Regression relies on the assumptions of linearity, independence, homoscedasticity, and normality of residuals, with an additional focus on addressing multicollinearity through regularization.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e802b379-f095-447c-94fb-c5f343205098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ef24fd9-6fee-4a81-91c8-5dec83a14568",
   "metadata": {},
   "source": [
    "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d375df-3417-479f-92b0-f8624d9e162d",
   "metadata": {},
   "source": [
    "Selecting the value of the tuning parameter (λ) in Ridge Regression is crucial for balancing the trade-off between fitting the training data and controlling the magnitude of the coefficients. Here are common methods for selecting λ:\n",
    "\n",
    "1)Cross-Validation:\n",
    "\n",
    "k-Fold Cross-Validation: Split the data into k subsets. Train the model on k-1 subsets and validate it on the remaining subset. Repeat this process k times, and average the performance metrics. Choose λ that minimizes the cross-validation error.\n",
    "Leave-One-Out Cross-Validation (LOOCV): Use each data point once as a validation set while training on the remaining points. This method is computationally intensive but can provide a precise estimate.\n",
    "\n",
    "2)Grid Search:\n",
    "\n",
    "Define a range of λ values and evaluate the model performance for each value using cross-validation. Choose the λ that results in the best performance metric (e.g., lowest mean squared error).\n",
    "\n",
    "3)Regularization Path Algorithms:\n",
    "\n",
    "Algorithms like the Least Angle Regression (LARS) with L2 regularization can compute the regression path for a sequence of λ values efficiently. You can select the λ that provides optimal performance based on the validation set.\n",
    "\n",
    "4)Information Criteria:\n",
    "\n",
    "Use criteria like the Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC), which balance model fit and complexity. These criteria can help select an appropriate λ by penalizing more complex models.\n",
    "\n",
    "5)Domain Knowledge:\n",
    "\n",
    "Sometimes, domain knowledge or prior experience with similar problems can guide the selection of λ. This is less formal but can be useful in practice.\n",
    "In summary, λ is typically selected using cross-validation, grid search, or regularization path algorithms, with the goal of finding the value that provides the best balance between model fit and regularization.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c6a81-8c71-4f17-801e-6e51427160bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3097062-c768-410a-a15b-b4d6f7a67ce1",
   "metadata": {},
   "source": [
    "Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f87c60-f9da-448d-a07b-1be1ea1db9a3",
   "metadata": {},
   "source": [
    "Ridge Regression is generally not used for feature selection in the same way as methods like Lasso Regression. Here's why and how it can still be useful:\n",
    "\n",
    "1)Feature Shrinkage, Not Elimination:\n",
    "\n",
    "Ridge Regression adds an L2 regularization term that penalizes the sum of the squares of the coefficients. This results in smaller coefficient values but does not force any coefficients to be exactly zero. Thus, all features are retained in the model, albeit with reduced impact.\n",
    "\n",
    "2)Handling Multicollinearity:\n",
    "\n",
    "While Ridge Regression does not perform feature selection, it is useful for dealing with multicollinearity by shrinking the coefficients, which stabilizes the estimates and improves model performance.\n",
    "\n",
    "3)Comparison with Lasso Regression:\n",
    "\n",
    "Unlike Lasso Regression, which uses L1 regularization and can set some coefficients to zero, Ridge Regression only shrinks coefficients. If feature selection is required, Lasso Regression or Elastic Net (which combines L1 and L2 penalties) might be more appropriate.\n",
    "\n",
    "4)Regularization Path Analysis:\n",
    "\n",
    "Although Ridge Regression doesn’t perform feature selection, analyzing the regularization path (how coefficients change with different λ values) can provide insights into the relative importance of features.\n",
    "\n",
    "In summary, Ridge Regression reduces the impact of less important features but does not eliminate them. For explicit feature selection, Lasso Regression or Elastic Net would be better choices.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3edbf9-061f-4995-8a7e-2cb85bae5c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a4c4813-f450-4817-ad77-ef613f0a5d8c",
   "metadata": {},
   "source": [
    "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981ff19a-a9c7-448f-b4b8-aa7b65a21109",
   "metadata": {},
   "source": [
    "In the presence of multicollinearity, Ridge Regression performs well by addressing some of the issues associated with it. Here's how:\n",
    "\n",
    "1)Stabilizes Coefficients:\n",
    "\n",
    "Multicollinearity occurs when predictors are highly correlated, leading to unstable coefficient estimates in ordinary least squares (OLS) regression. Ridge Regression adds L2 regularization, which penalizes the size of the coefficients. This regularization helps stabilize the estimates by shrinking them, making the model less sensitive to the multicollinearity among predictors.\n",
    "\n",
    "2)Improves Model Performance:\n",
    "\n",
    "By shrinking the coefficients, Ridge Regression reduces the variance of the estimates, which can improve the model's performance and generalization to new data. This makes the model more robust compared to OLS, which may perform poorly in the presence of multicollinearity.\n",
    "\n",
    "3)Does Not Eliminate Features:\n",
    "\n",
    "Unlike Lasso Regression, which can force some coefficients to zero, Ridge Regression keeps all features in the model but reduces their impact. This can be beneficial when all predictors have some level of importance, even if they are highly correlated.\n",
    "\n",
    "4)Enhances Predictive Accuracy:\n",
    "\n",
    "Ridge Regression generally leads to better predictive accuracy when multicollinearity is present, as it reduces the model's variance and improves its robustness, though it may come at the cost of introducing a small bias.\n",
    "\n",
    "In summary, Ridge Regression effectively handles multicollinearity by stabilizing coefficient estimates and improving model performance, though it does not perform feature selection or eliminate predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e01385e-6659-410d-b67c-50e4da31d1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1e585b3-3243-4c1e-a821-011a89c2c416",
   "metadata": {},
   "source": [
    "Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b6c05f-6272-49fe-ae72-491bcb430f65",
   "metadata": {},
   "source": [
    "Yes, Ridge Regression can handle both categorical and continuous independent variables. However, there are some considerations and steps you need to take when dealing with categorical variables in Ridge Regression.\n",
    "\n",
    "Categorical variables need to be appropriately encoded before they can be used in Ridge Regression, as it is a numerical optimization technique.\n",
    "\n",
    "Methods to handle categorical variables:\n",
    "\n",
    "1)Continuous Variables: Continuous independent variables can be used directly in Ridge Regression without any special transformation. The coefficients for continuous variables represent the change in the response variable associated with a one-unit change in the predictor variable, while keeping other variables constant.\n",
    "\n",
    "2)Categorical Variables: Categorical variables need to be converted into numerical form using techniques like one-hot encoding. Each category of a categorical variable is transformed into a binary (0 or 1) variable. For example, if you have a categorical variable \"Color\" with values \"Red,\" \"Blue,\" and \"Green,\" you would create three binary dummy variables: \"Color_Red,\" \"Color_Blue,\" and \"Color_Green.\" Ridge Regression treats these binary dummy variables as any other continuous variables in the model.\n",
    "\n",
    "3)Regularization for Dummy Variables: Ridge Regression applies regularization to all predictor variables, including the dummy variables created for categorical variables. This ensures that the model's coefficients are controlled, preventing overfitting and balancing the influence of the variables.\n",
    "\n",
    "4)Scaling: It's generally a good practice to standardize your continuous variables (mean = 0, standard deviation = 1) before using Ridge Regression. This ensures that the regularization term treats all variables equally.\n",
    "\n",
    "5)Intercept Term: Remember to include an intercept (constant) term in the model. The intercept represents the baseline value of the response variable when all predictor variables are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317bada3-527f-45d3-aa7e-2da0103e82aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff6b7b73-dd41-45e3-90c8-9e6ffd3b7aff",
   "metadata": {},
   "source": [
    "Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee7e28e-d032-4389-93d2-935b1d27a7c6",
   "metadata": {},
   "source": [
    "Interpreting the coefficients of Ridge Regression involves understanding their role and behavior in the context of regularization:\n",
    "\n",
    "1)Magnitude and Shrinkage:\n",
    "\n",
    "Ridge Regression applies L2 regularization, which shrinks the coefficients towards zero but does not set any coefficients exactly to zero. The magnitude of the coefficients indicates their relative importance, with smaller coefficients suggesting less influence on the dependent variable.\n",
    "\n",
    "2)Impact of Regularization:\n",
    "\n",
    "The regularization parameter (λ) controls the extent of shrinkage. Larger values of λ lead to greater shrinkage, which can reduce the impact of the coefficients. As λ increases, coefficients become smaller, reflecting a trade-off between fitting the data and keeping the model simpler.\n",
    "\n",
    "3)Relative Importance:\n",
    "\n",
    "Coefficients should be interpreted relative to each other rather than in absolute terms. Since Ridge Regression shrinks all coefficients, their sizes reflect their importance relative to other predictors in the model, but not their exact contribution.\n",
    "\n",
    "4)Multicollinearity Handling:\n",
    "\n",
    "Ridge Regression is effective in handling multicollinearity by reducing the impact of correlated predictors. The coefficients provide insights into the adjusted influence of predictors after accounting for multicollinearity.\n",
    "\n",
    "5)Comparison to OLS:\n",
    "\n",
    "Coefficients from Ridge Regression are generally smaller than those from Ordinary Least Squares (OLS) due to regularization. While OLS coefficients can be highly variable with multicollinearity, Ridge coefficients are more stable and reliable.\n",
    "\n",
    "In summary, Ridge Regression coefficients represent the impact of each predictor after accounting for regularization. They are shrunk towards zero, with their relative sizes reflecting the predictors' importance while addressing issues like multicollinearity.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca7e624-6ac1-452a-beba-daad225999a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fe9923e-bd05-4ec4-8083-6a222b0d6df7",
   "metadata": {},
   "source": [
    "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb794db6-fae1-429a-bb85-7f955849dab3",
   "metadata": {},
   "source": [
    "Yes, Ridge Regression can be used for time-series data analysis, although it's not the most common approach for handling time-series data. Time-series data has its own characteristics and challenges, such as autocorrelation, trend, and seasonality, which require specialized techniques. However, Ridge Regression can be adapted for time-series analysis with some modifications.\n",
    "\n",
    "Ways to use Ridge Regression for time-series data:\n",
    "\n",
    "1)Feature Engineering: For time-series data, you might need to engineer relevant features that capture trends, seasonality, and autocorrelation. These features can be used as predictors in the Ridge Regression model.\n",
    "\n",
    "2)Lagged Variables: Include lagged versions of the dependent variable and other relevant variables as predictors. This captures the time dependencies present in time-series data.\n",
    "\n",
    "3)Regularization: Ridge Regression can help prevent overfitting and stabilize coefficient estimates. It's particularly useful when you have a limited amount of data and are concerned about model complexity.\n",
    "\n",
    "4)Scaling: Standardize your continuous variables before using Ridge Regression to ensure that the regularization term affects all variables equally.\n",
    "\n",
    "5)Tuning λ: Choose an appropriate value for the regularization parameter λ through techniques like cross-validation. The right λ can help balance model complexity and fit for time-series data.\n",
    "\n",
    "6)Sequential Nature: Ridge Regression doesn't inherently account for the sequential nature of time-series data. You might need to modify the model or incorporate additional techniques to account for the order and dependencies of observations.\n",
    "\n",
    "7)Assumptions: Keep in mind that the assumptions of Ridge Regression, such as independence of errors, might not hold in the context of time-series data. There are other time-series-specific models, such as ARIMA, SARIMA, and more advanced models like state space models and recurrent neural networks, that are better suited to capture the dynamics of time-series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9bea54-6c37-47f2-b029-b7ee6d60b9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98257d3-4893-4aed-9f9c-c72f49ab47f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503d49b0-cb5f-4d56-9a5c-c055d90909c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc99b2-5be9-4b59-a3ed-4faca5b1491f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae6ce97-08f3-4864-8f44-121cdfee7b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b422ad-ba0d-45cf-9b3e-58c295d7b572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaecba8-4064-4657-ad21-012c9c96a490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe78d1d-a13e-4002-80ff-dce3a3aa6072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1fec37-7bb7-4e84-8560-20c0770ff9af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de3798-f048-4d2d-ac88-6c7cc6d04c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a51b9b-5e43-4f12-ba2a-d08ff8f85c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c0edc-2201-4aac-9d78-6385a75f1a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6269db97-0d53-434b-afca-2e99a0c7e9ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c5bff-e2aa-4446-b0b4-2f16372acc62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc3501-8fd5-46c4-8580-d73b0e1344f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ece2d-0b4b-4890-8bc7-6052d4bd2f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e69213-f849-407b-8fce-36d31e8adb68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01717e3b-4dbc-466c-928b-4977aca9f436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a87dda3-cfa6-4b20-affc-9cbfc9108010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc082ab8-c4c5-4728-b2d3-4bdbf535a350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21ce757-5e77-4b98-8dc7-89663e8bc3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e5e08e-1edf-4f9a-ac9b-50e31ff98865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0160c7dc-10d3-45e4-9318-16ef33275088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bbf7ef-9254-460c-b84e-9b9c3fee3d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f3a60-28fa-43a8-8fdc-5e17bb08d2db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060d3d6b-5255-4682-850a-5d306cb053e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2493de8d-6faa-43a9-97bb-bf134828d9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64131130-b994-463b-87aa-ea964ea6de66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c90d7d7-02d1-43e2-b0e0-8e6614150369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd9dad2-3993-4a4c-a08d-ea9528ea89f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08510102-a247-46a9-81b0-f13543e6ad51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a099d8-4cef-4662-b8cd-3286c268928d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e171902-9d3b-401e-a33e-f239d18c0691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19280ee8-ed39-46c0-aac2-e9b5ccbb7d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab1651f-556f-48ae-876b-6446b9e757e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd9699-d79c-40dd-9d96-ed757bb58361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
